{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8d806",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matminer.datasets.convenience_loaders import load_elastic_tensor\n",
    "from matminer.featurizers.conversions import StrToComposition, CompositionToOxidComposition\n",
    "from matminer.featurizers.composition import ElementProperty, OxidationStates\n",
    "# from matminer.featurizers.structure import DensityFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf68a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Data and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a54ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_pec_data = pd.read_excel('PCE1.xlsx', header = 1)\n",
    "# df_pec_data = df_pec_data.sort_values(['Sample'], ignore_index = True)\n",
    "col_labels = ['Element', 'Highest Ratio over Control', 'Average Ratio over Control', 'Concentration']\n",
    "df_pec_data.columns = col_labels\n",
    "df_pec_data.dropna(axis=0, how='all', inplace=True)\n",
    "df_pec_data = df_pec_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34e649",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Clean Duplicates and Keep Max Power Entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeddf3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# row_i = 0\n",
    "# formula_list = []\n",
    "# for row_i in range(len(df_pec_data)):\n",
    "#     formula = ''\n",
    "#     for col_j in np.arange(1,8,1):\n",
    "#         elm_frac = np.around(df_pec_data.iloc[row_i].values[col_j], 2)      #保留两位小数\n",
    "#         if elm_frac > 0:\n",
    "#             formula+=col_labels[col_j]+str(elm_frac)        \n",
    "#     formula_list.append(formula)\n",
    "df_pec_data['formula'] = df_pec_data['Element']   #为pd数据格式加了一列formula数据\n",
    "df_pec_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee153d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "P_cal_form = 'Average Ratio over Control' #'Average Ratio over Control'\n",
    "df_pec_data_sorted = df_pec_data.sort_values(by=['formula', P_cal_form],ascending = True) #升序排列\n",
    "\n",
    "df_pec_data_sorted.head(-10)        #why not make it show last 10 row(X delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8dcb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df_pec_data_cleaned = df_pec_data_sorted.drop_duplicates('formula', keep='last',ignore_index=True)  #pd格式下按相同的formula过滤，并只留下最后一个P最大的\n",
    "df_pec_data_cleaned = df_pec_data_sorted        #不去除同组成的data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91f6fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_pec = StrToComposition().featurize_dataframe(df_pec_data_cleaned, \"formula\") #似乎是按前几列的colum label整理target col的数据，输出含有前几列的label，并作为新的col添加到pd中\n",
    "df_pec.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc808f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd9172",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def y_inverse_transform(y_list):\n",
    "    #y_orignal = np.exp(std_scalery.inverse_transform(y_list.reshape(-1,1)))\n",
    "    #y_orignal = std_scalery.inverse_transform(y_list.reshape(-1,1))\n",
    "    y_orignal = y_list.reshape(-1,1)\n",
    "    return y_orignal \n",
    "\n",
    "def plt_true_vs_pred(y_true_list, y_pred_list, y_uncer_list, title_str_list, color_list,\n",
    "                      only_value=False,\n",
    "                      criterion='correlation'):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(5.5*2, 4.5))\n",
    "    fs = 20\n",
    "    scores, cal_methods = [], []\n",
    "    for i in np.arange(len(axes)):\n",
    "        ## inverse transform\n",
    "        y_true = y_true_list[i][:, -1]\n",
    "        y_pred = y_pred_list[i][:, -1]\n",
    "        y_uncer = np.sqrt(y_uncer_list[i][:,-1])\n",
    "\n",
    "        if 'correlation' in criterion:\n",
    "            spearman = spearmanr(y_true, y_pred) [0]\n",
    "            pearson = pearsonr(y_true, y_pred) [0]\n",
    "            scores.append([spearman, pearson])\n",
    "            cal_methods.append(['sp_r', 'p_r'])\n",
    "        elif'value' in criterion:\n",
    "            rmse_value = np.sqrt(mse(y_true, y_pred))\n",
    "            # mae_value = mae(y_true, y_pred)\n",
    "            mape_value = mape(y_true, y_pred)\n",
    "            scores.append([rmse_value, mape_value])\n",
    "            cal_methods.append(['rmse', 'mape'])\n",
    "        else:\n",
    "            raise TypeError(\"Invalid input. OPT: 'correlation', 'value' \")\n",
    "\n",
    "        if not only_value:\n",
    "            lims1 = (0*0.9, 3*1.1)\n",
    "            axes[i].scatter(y_true, y_pred, alpha = 0.3, c = color_list[i])\n",
    "            axes[i].errorbar(y_true, y_pred, yerr = y_uncer, ms = 0,\n",
    "                            ls = '', capsize = 2, alpha = 0.6,\n",
    "                            color = 'gray', zorder = 0)\n",
    "            axes[i].plot(lims1, lims1, 'k--', alpha=0.75, zorder=0)\n",
    "\n",
    "            title = title_str_list[i] + \" ({}={}, {}={})\".format(cal_methods[i][0], np.round(scores[i][0],2),\n",
    "                                                                 cal_methods[i][1], np.round(scores[i][1],2))\n",
    "            axes[i].set_xlabel('Ground Truth', fontsize = fs)\n",
    "            axes[i].set_ylabel('Prediction', fontsize = fs)\n",
    "            axes[i].set_title(title, fontsize = fs)\n",
    "            axes[i].set_xlim(0.5 , 1.5)\n",
    "            axes[i].set_ylim(0.5 , 1.5)\n",
    "            axes[i].tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "            axes[i].grid(True, linestyle='-.')\n",
    "\n",
    "    if not only_value:\n",
    "        plt.subplots_adjust(wspace = 0.35)\n",
    "        plt.show()\n",
    "\n",
    "    assert len(cal_methods) == len(scores)\n",
    "    return cal_methods, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a24e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build regression model with composition descriptors (from `matminer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c87cf5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ep_feat = ElementProperty.from_preset(preset_name=\"magpie\")\n",
    "df_pec_magpie = ep_feat.featurize_dataframe(df_pec, col_id=\"composition\")  #这两行是matminer的固定操作，用于加入描述符col\n",
    "# input the \"composition\" column to the featurizer\n",
    "df_pec_magpie.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2ac68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = df_pec_magpie.shape[1] - 132           # changed param1 \n",
    "use_concentration = True\n",
    "# if use_concentration: \n",
    "desc = pd.concat([ df_pec_magpie['Concentration'], df_pec_magpie.iloc[:, _:] ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0f33c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_compo = desc.values  # all descriptors\n",
    "# eval('P_cal_form')\n",
    "y_pmax = df_pec_magpie[ P_cal_form ].values     #P（Eff max）\n",
    "# y_pmax \n",
    "# X_compo.shape\n",
    "# y_pmax.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b14c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "std_scalerX = MinMaxScaler()            #用于进行col数据的归一化（norm1）到[0,1]之间，是按列进行norm（将数据的每一个属性值减去其最小值，然后除以其极差）\n",
    "std_scalery = StandardScaler()          #是一个用来对数据进行归一化和标准化的类norm2（利用var std等（那么在预测的时候， 也要对数据做同样的标准化处理，即也要用上面的scaler中的均值和方差来对预测时候的特征进行标准化\n",
    "\n",
    "X = np.array(X_compo)\n",
    "#X_log = np.log(X.astype('float'))   \n",
    "y = np.array(y_pmax.reshape(-1,1))      \n",
    "\n",
    "mi = mutual_info_regression(X, y)   #[616, 132] VS [616, 1]\n",
    "mi /= np.max(mi)                    #还是norm3\n",
    "plt.bar(np.arange(len(mi)), mi)     #mi 为132项， 画图是为了看mat desc中哪项对于Pmax的关联性最大\n",
    "plt.show()\n",
    "\n",
    "# hyperparam 2 :\n",
    "thr = 0.2                                   #暂时不用mi去确定参与降维的维度的idx了？\n",
    "# np.count_nonzero(mi>thr)        #32\n",
    "\n",
    "from sklearn.decomposition import PCA       #mi用于过滤，使高维向量缩减到相关性强的维度上\n",
    "# pca = PCA(n_components=20)                   ###n_components=20 must be between 0 and min(n samples,n features)=8\n",
    "pca = PCA(n_components=0.9999)                  #使用：则会被降到5维\n",
    "x_pca = pca.fit_transform(X)                    #PCA之前是否需要StandardScaler norm一下（和原论文中顺序不同）？\n",
    "X_norm = std_scalerX.fit_transform(x_pca)       #重新对pca后的x进行归一化 norm4     \n",
    "\n",
    "# y_log = np.log(y.astype('float'))   \n",
    "#y_norm =  std_scalery.fit_transform(y)\n",
    "X_norm.shape\n",
    "# mi.shape                                      #plot显示似乎是 一个维度的相邻的维度 与Pmax的关系比较大\n",
    "# print (pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1a0cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n_plots = 5\n",
    "fs = 20\n",
    "# set the font name for a font family\n",
    "plt.rcParams.update({'font.sans-serif':'Helvetica'})\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(5.5*n_plots, 4))\n",
    "\n",
    "for i in np.arange(n_plots):\n",
    "    axes[i].hist(np.array(x_pca[:,i]), bins =20,                                                #画hist是为了查看X某个维度(20个里面找一个）的数据分布情况\n",
    "                 width = 0.04*(np.max(np.array(x_pca[:,i]))-np.min(np.array(x_pca[:,i]))),          #x_pca\n",
    "                 alpha = 0.7)\n",
    "    axes[i].tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "    axes[i].grid(True, linestyle='-.')\n",
    "    axes[i].set_xlabel('Values', fontsize = fs)\n",
    "    axes[i].set_ylabel('Counts', fontsize = fs)\n",
    "    axes[i].set_title('Raw X'+str(i+1), fontsize = fs)\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.35, \n",
    "                    hspace=0.35)\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(5.5*n_plots, 4))\n",
    "fs = 20\n",
    "\n",
    "for i in np.arange(n_plots):\n",
    "    axes[i].hist(X_norm[:,i], bins =20, \n",
    "                 width = 0.04*(np.max(np.array(X_norm[:,i]))-np.min(np.array(X_norm[:,i]))),        #X_norm\n",
    "                 alpha = 0.7, color = 'orange')\n",
    "    axes[i].tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "    axes[i].grid(True, linestyle='-.')\n",
    "    axes[i].set_xlabel('Values', fontsize = fs)\n",
    "    axes[i].set_ylabel('Counts', fontsize = fs)\n",
    "    axes[i].set_title('Normalized X'+str(i+1), fontsize = fs)\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.35, \n",
    "                    hspace=0.35)\n",
    "plt.show()      #注意hist count的总和为616\n",
    "#这里图是空白应该是因为有些维度的数据都是0\n",
    "X[:,2]  #X_norm[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078520a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "sel_train_elem"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)       #固定seed\n",
    "num_elements = np.array([len(df_pec.composition[ci]) for ci in np.arange(len(df_pec))]) #提取616行中compsition元素的个数\n",
    "sel_indx_1elem = (num_elements==1) \n",
    "sel_indx_2elem = (num_elements==2)      \n",
    "sel_indx_3elem = (num_elements==3)\n",
    "sel_indx_4elem = (num_elements==4)\n",
    "# X_inp_list = [X_norm[sel_indx_2elem], X_norm[sel_indx_3elem], X_norm[sel_indx_4elem]]\n",
    "X_inp_list = [X_norm[sel_indx_1elem]]\n",
    "# y_outp_list = [y[sel_indx_2elem], y[sel_indx_3elem], y[sel_indx_4elem]]\n",
    "y_outp_list = [y[sel_indx_1elem]]\n",
    "# print(np.count_nonzero(sel_indx_1elem))\n",
    "elem1_indx_random = [np.random.randint(len(X_norm[sel_indx_1elem])) for i in np.arange(np.count_nonzero(sel_indx_1elem))]   #用所有该集合的dp训练\n",
    "\n",
    "## elem4_indx_random = [np.random.randint(len(X_norm[sel_indx_4elem])) for i in np.arange(20)]\n",
    "## print('num of samples:', np.sum(sel_indx))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inp_list[0][elem1_indx_random],   #只对num_elements==3的数据进行训练(并且只用其中随机抽取的20个元素)\n",
    "                                                    y_outp_list[0][elem1_indx_random],  #\n",
    "                                                    test_size = 0.2)#, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)  #[16, 20]  [4, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69a8dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import GPy \n",
    "from GPy.models import GPRegression\n",
    "## \n",
    "ker = GPy.kern.Matern52(input_dim = len(X_train[0]), ARD =True)     #Matern52有啥讲究吗？\n",
    "ker.lengthscale.constrain_bounded(1e-2, 25)         #超参数？（好像是posterior 得到的）\n",
    "ker.variance.constrain_bounded(1e-2, 100.0)\n",
    "\n",
    "gpy_regr = GPRegression(X_train, y_train, ker)#\n",
    "#gpy_regr.Gaussian_noise.variance = (0.01)**2       #这个一般需要怎么调整呢？（好像是posterior 得到的）\n",
    "#gpy_regr.Gaussian_noise.variance.fix()\n",
    "\n",
    "gpy_regr.randomize()\n",
    "gpy_regr.optimize_restarts(num_restarts=20, verbose =True, messages=False)\n",
    "\n",
    "# display(gpy_regr)\n",
    "# GPy.plotting.show(m.plot(), filename='RBF_001')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ker.lengthscale, '/n')\n",
    "print(ker.variance, '/n')\n",
    "print(gpy_regr.Gaussian_noise)\n",
    "\n",
    "y_pred_train, y_uncer_train= gpy_regr.predict(X_train)\n",
    "y_pred_test, y_uncer_test = gpy_regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b388b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "error?"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "plt_true_vs_pred([y_train, y_test],\n",
    "                 [y_pred_train, y_pred_test],[y_uncer_train, y_uncer_test],\n",
    "                 ['Mat52-Train','Mat52-Test'],\n",
    "                 ['blue', 'darkorange'], criterion='correlation') \n",
    "# cal_method, scores = plt_true_vs_pred([y_train_fold, y_test_fold], \n",
    "#                  [y_pred_train, y_pred_test],[y_uncer_train, y_uncer_test],\n",
    "#                  ['GP-Mat52 - Train','GP-Mat52 - Test'],\n",
    "#                  ['blue', 'darkorange'], only_value=True, criterion='correlation') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8049b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import GPy \n",
    "from GPy.models import GPRegression\n",
    "\n",
    "# when use K fold not use train_test split: \n",
    "X_train = X_norm; y_train = y[:, -1]\n",
    "# 创建一个用于得到不同训练集和测试集样本的索引的StratifiedKFold实例，折数为5\n",
    "strtfdKFold = KFold(n_splits=5)\n",
    "#把特征和标签传递给StratifiedKFold实例\n",
    "kfold = strtfdKFold.split(X_train, y_train)\n",
    "#循环迭代，（K-1）份用于训练，1份用于验证，把每次模型的性能记录下来。\n",
    "scores_train, scores_test = [], []\n",
    "uncer_train, uncer_test = [], []\n",
    "\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    X_train_fold, y_train_fold = X_train[train], y_train[train]\n",
    "    X_test_fold, y_test_fold = X_train[test], y_train[test]\n",
    "\n",
    "    ker = GPy.kern.Matern52(input_dim = len(X_train_fold[0]), ARD =True)     #Matern52有啥讲究吗？\n",
    "    ker.lengthscale.constrain_bounded(1e-2, 25)         #超参数？（好像是posterior 得到的）\n",
    "    ker.variance.constrain_bounded(1e-2, 100.0)\n",
    "\n",
    "    gpy_regr = GPRegression(X_train_fold, y_train_fold.reshape(-1,1), ker)#\n",
    "    #gpy_regr.Gaussian_noise.variance = (0.01)**2       #这个一般需要怎么调整呢？（好像是posterior 得到的）\n",
    "    #gpy_regr.Gaussian_noise.variance.fix()\n",
    "\n",
    "    y_pred_train, y_uncer_train = gpy_regr.predict(X_train_fold)\n",
    "    y_pred_test, y_uncer_test = gpy_regr.predict(X_test_fold)\n",
    "\n",
    "    y_pred_train, y_uncer_train = y_pred_train[:,-1], y_uncer_train[:,-1]\n",
    "    y_pred_test, y_uncer_test = y_pred_test[:,-1], y_uncer_test[:,-1]\n",
    "\n",
    "    gpy_regr.randomize()\n",
    "    gpy_regr.optimize_restarts(num_restarts=5, verbose=False, messages=False)\n",
    "    \n",
    "    score_train = pearsonr(y_train_fold, y_pred_train) [0]; scores_train.append(score_train)\n",
    "    score_test = pearsonr(y_test_fold, y_pred_test) [0]; scores_test.append(score_test)\n",
    "    # train_score = spearmanr(y_train_fold, y_pred_train) [0]\n",
    "    uncer_train.append(y_uncer_train.mean())\n",
    "    uncer_test.append(y_uncer_test.mean())\n",
    "\n",
    "# plot_CrossVal_avg(uncer_train, uncer_test)\n",
    "# plot_CrossVal_avg(scores_train, scores_test)\n",
    "print('\\n\\nTRAIN: Cross-Validation score: %.3f +/- %.3f' %(np.array(scores_train).mean(), \n",
    "                                                            np.array(scores_train).std()))\n",
    "print('\\n\\nTEST: Cross-Validation score: %.3f +/- %.3f' %(np.array(scores_test).mean(), \n",
    "                                                            np.array(scores_test).std()))\n",
    "print('\\n\\nTRAIN: Cross-Validation uncertainty: %.3f +/- %.3f' %(np.array(uncer_train).mean(), \n",
    "                                                            np.array(uncer_train).std()))\n",
    "print('\\n\\nTEST: Cross-Validation uncertainty: %.3f +/- %.3f' %(np.array(uncer_test).mean(),\n",
    "                                                            np.array(uncer_test).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51855f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d41121",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_val = X_inp_list[2]## 4 Element Dataset\n",
    "y_val = y_outp_list[2]## 4 Element Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab9a5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_val, y_uncer_val = gpy_regr.predict(X_val)\n",
    "plt_true_vs_pred([y_test, y_val], \n",
    "                 [y_pred_test, y_pred_val], [y_uncer_test, y_uncer_val],\n",
    "                 ['GP-Mat52 - Test','GP-Mat52 - Valid'],\n",
    "                 ['darkorange', 'darkred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9e18e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.sans-serif':'Helvetica'})\n",
    "n_plots = 2\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(3.5*n_plots, 3))\n",
    "y_hist = [y_test, y_val]                                  ####这里是做了一个对比图：为了查看验证集和测试集中pred和ground truth数值的分布情况\n",
    "color = ['orange', 'red']\n",
    "hist_labels = ['test GT', 'validation GT']\n",
    "for i in np.arange(n_plots):\n",
    "    axes[i].hist(np.array(y_hist[i]), bins =20, \n",
    "                 width = 0.04*(np.max(np.array(y_hist[i]))-np.min(np.array(y_hist[i]))), \n",
    "                 alpha = 0.7, color = color[i])\n",
    "    axes[i].tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "    axes[i].grid(True, linestyle='-.')\n",
    "    axes[i].set_xlabel('Values', fontsize = fs)\n",
    "    axes[i].set_ylabel('Counts', fontsize = fs)\n",
    "    axes[i].set_title(hist_labels[i], fontsize = fs)\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.35, \n",
    "                    hspace=0.35)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.sans-serif':'Helvetica'})\n",
    "n_plots = 2\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(3.5*n_plots, 3))\n",
    "y_hist = [y_pred_test, y_pred_val]\n",
    "color = ['darkorange', 'darkred']\n",
    "hist_labels = ['test pred', 'validation pred']\n",
    "for i in np.arange(n_plots):\n",
    "    axes[i].hist(np.array(y_hist[i]), bins =20, \n",
    "                 width = 0.04*(np.max(np.array(y_hist[i]))-np.min(np.array(y_hist[i]))), \n",
    "                 alpha = 0.7, color = color[i])\n",
    "    axes[i].tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "    axes[i].grid(True, linestyle='-.')\n",
    "    axes[i].set_xlabel('Values', fontsize = fs)\n",
    "    axes[i].set_ylabel('Counts', fontsize = fs)\n",
    "    axes[i].set_title(hist_labels[i], fontsize = fs)\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.35, \n",
    "                    hspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c814143",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# elem4_indx_random = [np.random.randint(len(X_val)) for i in np.arange(10)]\n",
    "# X_val_init = X_val[elem4_indx_random]\n",
    "# y_val_init = y_val[elem4_indx_random]\n",
    "# X_init = np.concatenate([X_train, X_test, X_val_init])      #从elem4中挑选一部分（10个）数据参与model训练\n",
    "# y_init = np.concatenate([y_train, y_test, y_val_init])\n",
    "# X_remain = np.delete(X_val,elem4_indx_random, 0)\n",
    "# y_remain = np.delete(y_val,elem4_indx_random, 0)\n",
    "# len(y_remain)\n",
    "\n",
    "# for it in np.arange(20):\n",
    "#     print(\"highest power so far: \", np.max(y_init))\n",
    "\n",
    "#     ker = GPy.kern.Matern52(input_dim = len(X_init[0]), ARD =True)#\n",
    "#     ker.lengthscale.constrain_bounded(1e-2, 10)\n",
    "#     ker.variance.constrain_bounded(1e-2, 100.0)\n",
    "\n",
    "#     gpy_regr = GPRegression(X_init, y_init, ker)#\n",
    "#     #gpy_regr.Gaussian_noise.variance = (0.01)**2\n",
    "#     #gpy_regr.Gaussian_noise.variance.fix()\n",
    "#     gpy_regr.randomize()\n",
    "#     gpy_regr.optimize_restarts(num_restarts=3,verbose =True, messages=False)\n",
    "#     print(ker.lengthscale)\n",
    "#     print(ker.variance)\n",
    "#     print(gpy_regr.Gaussian_noise)\n",
    "\n",
    "\n",
    "#     y_pred_init, y_uncer_init= gpy_regr.predict(X_init)\n",
    "#     y_pred_remain, y_uncer_remain = gpy_regr.predict(X_remain)          #测试剩余的elem4元素的预测情况\n",
    "#     plt_true_vs_pred([y_init, y_remain], \n",
    "#                      [y_pred_init, y_pred_remain], [y_uncer_init, y_uncer_remain],\n",
    "#                      ['GP-Mat52 - Init','GP-Mat52 - Remain'],\n",
    "#                      ['darkorange', 'darkred'])\n",
    "#     ucb = np.sqrt(y_uncer_remain)+y_pred_remain             ##[468, 1]\n",
    "#     top_ucb_indx = np.argsort(ucb[:,-1])[-10:]              #np.argsort处理一维数组，返回从小到大的索引(这里选取P最大的10个dp)  \n",
    "#     X_new = X_remain[top_ucb_indx]                           #ucb[:,-1]相当于一种特殊的reshape，可以将[3,1]变为[3,]\n",
    "#     y_new = y_remain[top_ucb_indx]\n",
    "\n",
    "#     X_init = np.concatenate([X_init, X_new])\n",
    "#     y_init = np.concatenate([y_init, y_new])\n",
    "#     X_remain = np.delete(X_remain,top_ucb_indx, 0)\n",
    "#     y_remain = np.delete(y_remain,top_ucb_indx, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2caa8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 1, figsize=(5.5*1.5, 4.5), sharey = False)\n",
    "# fs = 20\n",
    "# ax = axes\n",
    "# ax.plot(np.arange(len(y_init))-len(np.concatenate([X_train, X_test])),    #含义为：当前使用的dp个数（包括elem4）减去所使用的elem3一共的数据个数\n",
    "#         np.maximum.accumulate(y_init), c = 'green', label = 'MM - ElemDescriptor')\n",
    "# ax.scatter(np.arange(len(y_init))-len(np.concatenate([X_train, X_test])),y_init, c = 'green',alpha = 0.2)\n",
    "\n",
    "# ax.plot(np.zeros(10),np.arange(10)/2-1, '--',c = 'black')\n",
    "# ax.set_ylabel('Current Best Efficiency', fontsize = 20)\n",
    "# ax.set_xlabel('Materials Composition', fontsize = 20)\n",
    "\n",
    "# ax.set_ylim(0.2, 3.5)\n",
    "# #axes[0].set_xlim(-1, 105)\n",
    "# #axes[0].set_xticks(np.arange(0,105,10))\n",
    "# ax.legend(fontsize = fs*0.7)\n",
    "# ax.tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "# ax.grid(True, linestyle='-.')\n",
    "\n",
    "# plt.show()                      #这图似乎是预测出的最大的P 和加入model训练的dp数量的关系图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d3554",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_list_descr = []                                   #此cell为重复之前的思路的总结版\n",
    "for rep in np.arange(10):                                      #10次cycle，这里相当于repeat 10次\n",
    "    elem4_indx_random = [np.random.randint(len(X_val)) for i in np.arange(10)]\n",
    "    X_val_init = X_val[elem4_indx_random]\n",
    "    y_val_init = y_val[elem4_indx_random]                   \n",
    "    X_init = np.concatenate([X_train, X_test, X_val_init])          #这里的X_init训练集包括所有的elem3以及5个elem4 dp\n",
    "    y_init = np.concatenate([y_train, y_test, y_val_init])\n",
    "    X_remain = np.delete(X_val,elem4_indx_random, 0)\n",
    "    y_remain = np.delete(y_val,elem4_indx_random, 0) \n",
    "    for it in np.arange(20):                                        #每次cycle中进行20次迭代（最终训练集size = init_X_size + 20*10)\n",
    "        print(\"highest power so far: \", np.max(y_init))             #源码中似乎是每次进行一次X_init的更新后进行一次pca（每个batch后）\n",
    "\n",
    "        ker = GPy.kern.Matern52(input_dim = len(X_init[0]), ARD =True)#\n",
    "        ker.lengthscale.constrain_bounded(1e-2, 10) \n",
    "        ker.variance.constrain_bounded(1e-2, 100.0)\n",
    "\n",
    "        gpy_regr = GPRegression(X_init, y_init, ker)#\n",
    "        #gpy_regr.Gaussian_noise.variance = (0.01)**2\n",
    "        #gpy_regr.Gaussian_noise.variance.fix()\n",
    "        gpy_regr.randomize()\n",
    "        gpy_regr.optimize_restarts(num_restarts=3,verbose =True, messages=False)\n",
    "        print(ker.lengthscale)\n",
    "        print(ker.variance)\n",
    "        print(gpy_regr.Gaussian_noise)\n",
    "\n",
    "\n",
    "        y_pred_init, y_uncer_init= gpy_regr.predict(X_init)\n",
    "        y_pred_remain, y_uncer_remain = gpy_regr.predict(X_remain)\n",
    "        plt_true_vs_pred([y_init, y_remain], \n",
    "                         [y_pred_init, y_pred_remain], [y_uncer_init, y_uncer_remain],\n",
    "                         ['GP-Mat52 - Init','GP-Mat52 - Remain'],\n",
    "                         ['darkorange', 'darkred'])\n",
    "        ucb = np.sqrt(y_uncer_remain)+y_pred_remain         #对计算标准有一点疑问？+ 最终model的预测准确度不做评判吗？+还是只能输入一组特定材料输出Pmax？\n",
    "        top_ucb_indx = np.argsort(ucb[:,-1])[-10:]\n",
    "        X_new = X_remain[top_ucb_indx]        #加入训练集的的是每次预测X_remain中的P最大的前10个（相当于选择性的将测试集中的数据加入训练）\n",
    "        y_new = y_remain[top_ucb_indx]\n",
    "'''抽样策略的评价标准很重要'''\n",
    "\n",
    "        X_init = np.concatenate([X_init, X_new])\n",
    "        y_init = np.concatenate([y_init, y_new])\n",
    "        X_remain = np.delete(X_remain,top_ucb_indx, 0)\n",
    "        y_remain = np.delete(y_remain,top_ucb_indx, 0)\n",
    "        print(len(X_init))\n",
    "    y_list_descr.append(y_init)                     #记录repeat 10次中每次repeat的最后参与训练的的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538aa516",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(5.5*1.5, 4.5), sharey = False)\n",
    "fs = 20\n",
    "ax = axes\n",
    "ymax_acc_list = []\n",
    "for i in np.arange(len(y_list_descr)):\n",
    "    ymax_acc = np.maximum.accumulate(y_list_descr[i])\n",
    "    ax.plot(np.arange(len(y_list_descr[i]))-len(np.concatenate([X_train, X_test])),     #画max线。len(y_list_descr[i])应该是相同的\n",
    "            ymax_acc, c = 'green', alpha = 0.3)\n",
    "    ymax_acc_list.append(ymax_acc)\n",
    "ax.plot(np.arange(len(y_list_descr[0]))-len(np.concatenate([X_train, X_test])),\n",
    "        np.mean(ymax_acc_list, axis = 0), '--', c = 'blue', alpha = 0.8)                 #repeat的10次的mean Pmax值\n",
    "#ax.scatter(np.arange(len(y_init))-len(np.concatenate([X_train, X_test])),y_init, c = 'green',alpha = 0.2)\n",
    "\n",
    "ax.plot(np.zeros(10),np.arange(10)/2-1, '--',c = 'black')\n",
    "ax.set_ylabel('Current Best Efficiency', fontsize = 20)\n",
    "ax.set_xlabel('Materials Composition', fontsize = 20)\n",
    "\n",
    "ax.set_ylim(0.2, 3.5)\n",
    "#axes[0].set_xlim(-1, 105)\n",
    "#axes[0].set_xticks(np.arange(0,105,10))\n",
    "ax.legend(fontsize = fs*0.7)\n",
    "ax.tick_params(direction='in', length=5, width=1, labelsize = fs*.8, grid_alpha = 0.5)\n",
    "ax.grid(True, linestyle='-.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53417591",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a0cfca14b4cd02aa22913b6532a941e15cba0b165a435a7f0598230cc935695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}